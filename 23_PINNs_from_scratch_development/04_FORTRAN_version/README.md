# Dense network with Adam optimizer algorithm
This is the well studied and implemented neural network code in
fortran with random normal initialization and adam optimization algorithm
for training the network.

Here, the SGD was first implemented and then extended to adam optimization.
SGD optimization requires bigger network to train than Adam optimization, that
was observed from the approximation of absorbance spectrum
